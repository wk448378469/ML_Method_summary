{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer 优化器\n",
    "\n",
    "其实就是更新参数（比如weights、biases等）的方法上的不同比如传统的随机梯度下降方法等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降,最基础的，应该会分为批梯度、随机梯度、全梯度三种吧，就是每次使用的样本数量不同\n",
    "# 缺点，挺多的\n",
    "# x = x - learning_rate * dx\n",
    "opt1 = tf.train.GradientDescentOptimizer(learning_rate=0.1,use_locking=False,name='GradientDescent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XXX提出的适应性学习率法，累加之前所有的梯度平方\n",
    "# cache += dx**2\n",
    "# x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "opt3 = tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=0.1, use_locking=False, name='Adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对Adagrad的扩展\n",
    "opt2 = tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name='Adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 估计也是对Adagrad的优化扩展\n",
    "opt4 = tf.train.AdagradDAOptimizer(learning_rate=0.01, global_step=1, initial_gradient_squared_accumulator_value=0.1, l1_regularization_strength=0.0, l2_regularization_strength=0.0, use_locking=False, name='AdagradDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 硬翻译的话是动量法？\n",
    "# 梯度长时间保持一个方向（为正或者长时间为负），则增大参数更新幅度，反之，如果频繁发生符号翻转，则说明这是要减小参数更新幅度\n",
    "# v = mu * v - learning_rate * dx\n",
    "# x += v\n",
    "opt5 = tf.train.MomentumOptimizer(learning_rate=1, momentum = 0.9, use_locking=False, name='Momentum', use_nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 亚当提出来的，所以就叫adam了。。。\n",
    "# m = beta1*m + (1-beta1)*dx\n",
    "# v = beta2*v + (1-beta2)*(dx**2)\n",
    "# x += - learning_rate * m / (np.sqrt(v) + eps)\n",
    "opt6 = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不懂～\n",
    "opt7 = tf.train.FtrlOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学习速率梯度均方根均值指数衰减，\n",
    "# cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "# x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "opt8 = tf.train.RMSPropOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不懂～～～\n",
    "opt9 = tf.train.ProximalAdagradOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 不懂～～～\n",
    "opt10 = tf.train.ProximalGradientDescentOptimizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
