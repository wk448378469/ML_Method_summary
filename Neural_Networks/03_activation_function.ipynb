{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_int = tf.Variable(np.random.randint(100,size=30))\n",
    "data_float = tf.Variable(np.random.rand(100).astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation function 激励函数\n",
    "\n",
    "在神经网络中，隐层和输出层节点的输入和输出之间具有函数关系，这个函数称为激励函数（有时也成为单元处理函数or压缩函数）。\n",
    "\n",
    "常见的激励函数有：线性激励函数、阈值或阶跃激励函数、S形激励函数、双曲正切激励函数和高斯激励函数等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = max(0,x)\n",
    "# x小于0时恒为0,大于0线性增长\n",
    "# 收敛速度快，计算开销小\n",
    "# 可能导致神经元永远不被激活\n",
    "tf.nn.relu(features=data_int,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = min(max(x,0),6)\n",
    "# 应该算作是relu的优化版本，但是好像用的不多\n",
    "tf.nn.relu6(features=data_float,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 应该是并行计算relu \n",
    "# 但是真的不太懂\n",
    "tf.nn.crelu(features=,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = exp(x) -1 if f(x) < 0 return x\n",
    "tf.nn.elu(features=,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = log(1+exp(x))\n",
    "# sigmoid 的原函数\n",
    "# 曲线上看是relu的平滑版本\n",
    "# 比relu慢，但是连续可微，且变化平稳，\n",
    "tf.nn.softplus(features=data_float,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = 1/(1+abs(x))\n",
    "# 连续非线性的\n",
    "# 这个表达式应该不是处处可微分吧？\n",
    "tf.nn.softsign(features=,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 看了相关的介绍应该稍微清楚点了，应该是下面这样的实施\n",
    "# H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "# U1 = np.random.rand(*H1.shape) < p \n",
    "# H1 = H1 × U1\n",
    "tf.nn.dropout(x=,keep_prob=,noise_shape=None,seed=None,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 添加偏差\n",
    "tf.nn.bias_add(value=data_float,bias=,data_format=None,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = 1 / (1 + exp(-x))\n",
    "# 反向传播的时候，容易杀掉gradients\n",
    "# 不以0为中心，且恒大于零\n",
    "# 指数运算开销大\n",
    "tf.sigmoid(x=data_float,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) = exp(x)-exp(-x) / exp(x) + exp(-x)\n",
    "# 克服了sigmoid的恒大于0的问题\n",
    "# 其他两个仍然是问题\n",
    "tf.tanh(x=data_float,name=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
